{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e145bfb8",
   "metadata": {},
   "source": [
    "# Autograd in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c46bf6",
   "metadata": {},
   "source": [
    "- During training we need to calculate the gradients of the loss function with respect to the weights of the model.\n",
    "- This is done using backpropagation.\n",
    "- Pytorch provides a very convenient way to calculate the gradients using autograd.\n",
    "- Autograd is a module in pytorch which provides automatic differentiation.\n",
    "- It is used to calculate the gradients of the loss function with respect to the weights of the model.\n",
    "- It is used in training of neural networks.\n",
    "- It is used in optimization of the weights of the model.\n",
    "- It is used in backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e9829",
   "metadata": {},
   "source": [
    "- The reason we need autograd is because to calculate the differentiation which needs chain rule, coding it manually is not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f109b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4622d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bbeac",
   "metadata": {},
   "source": [
    "requires_grad = True\n",
    "- It is used to tell pytorch that we want to calculate the gradient of this tensor.\n",
    "- So pytorch will track it, all operations we perform on this tensor and when we say it immediately calculate the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a78c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fcefd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e80119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b86a1",
   "metadata": {},
   "source": [
    "backward()\n",
    "- When pytorch makes a graph of the operations performed on the tensor.\n",
    "- In forward propagation or simply happened in calculations when y = x ** 2, as we print y it gives result 9 for x input 3.\n",
    "- But in backward propagation it calculates in differentiation of x means dy/dx = d(x**2)/dx = 2x = 2 * 3 = 6.\n",
    "\n",
    "- And this is the difference in forward and backward propagation.\n",
    "- We need to tell pytorch to backpropagate the gradient by calling backward() function.\n",
    "- This is how we can calculate gradient in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4f2794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97957bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2d17a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "514c4f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4121, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f40358",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deb35688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0845d4",
   "metadata": {},
   "source": [
    "- Here's a problem that x.grad just gives calculated value but if we use backward() more than one gives an error because one we use backward() in graph it is detached from the graph or can say freed from the graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

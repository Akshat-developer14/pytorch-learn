{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eff5e53",
   "metadata": {},
   "source": [
    "# Tensor Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5597d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x2 tensor with float values and enable gradient tracking.\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "print(\"Tensor x:\")\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124b40e",
   "metadata": {},
   "source": [
    "### Inspect Core Tensor Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3254dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Core Attributes of x --\n",
      "dtype:          torch.float32\n",
      "device:         cpu\n",
      "shape:          torch.Size([2, 2])\n",
      "requires_grad:  True\n",
      "grad:           None\n",
      "grad_fn:        None\n",
      "layout:         torch.strided\n",
      "is_leaf:        True\n"
     ]
    }
   ],
   "source": [
    "# Core attributes of tensor x (a leaf tensor):\n",
    "print(\"\\n-- Core Attributes of x --\")\n",
    "print(\"dtype:         \", x.dtype)\n",
    "print(\"device:        \", x.device)\n",
    "print(\"shape:         \", x.shape)       # Same as x.size()\n",
    "print(\"requires_grad: \", x.requires_grad)\n",
    "print(\"grad:          \", x.grad)        # None until a backward pass is run.\n",
    "print(\"grad_fn:       \", x.grad_fn)     # None because x is a leaf tensor.\n",
    "print(\"layout:        \", x.layout)\n",
    "print(\"is_leaf:       \", x.is_leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86823f",
   "metadata": {},
   "source": [
    "### Create a Non-Leaf Tensor and Check Its Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45e5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor y (result of x + 2):\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "-- Attributes of Tensor y --\n",
      "dtype:          torch.float32\n",
      "device:         cpu\n",
      "shape:          torch.Size([2, 2])\n",
      "requires_grad:  True\n",
      "grad:           None\n",
      "grad_fn:        <AddBackward0 object at 0x00000229B6AF2C20>\n",
      "layout:         torch.strided\n",
      "is_leaf:        False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_11096\\562828982.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"grad:          \", y.grad)        # Still None (no backward pass yet)\n"
     ]
    }
   ],
   "source": [
    "# Perform an operation on x to create y. Now y is not a leaf tensor.\n",
    "y = x + 2\n",
    "\n",
    "print(\"\\nTensor y (result of x + 2):\")\n",
    "print(y)\n",
    "\n",
    "print(\"\\n-- Attributes of Tensor y --\")\n",
    "print(\"dtype:         \", y.dtype)\n",
    "print(\"device:        \", y.device)\n",
    "print(\"shape:         \", y.shape)\n",
    "print(\"requires_grad: \", y.requires_grad)\n",
    "print(\"grad:          \", y.grad)        # Still None (no backward pass yet)\n",
    "print(\"grad_fn:       \", y.grad_fn)     # Shows the function that created y.\n",
    "print(\"layout:        \", y.layout)\n",
    "print(\"is_leaf:       \", y.is_leaf)       # False â€“ since it was created via an operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d296a",
   "metadata": {},
   "source": [
    "### Demonstrate Additional Useful Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28a73f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transpose of x (x.T):\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]], grad_fn=<PermuteBackward0>)\n",
      "\n",
      "Using x.data (detached view):\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "Is x on CUDA?  False\n",
      "Is x MKL-DNN?  False\n"
     ]
    }
   ],
   "source": [
    "# --- Additional Attributes ---\n",
    "\n",
    "# Transpose: For a 2D tensor, x.T is a shorthand for x.transpose(0, 1)\n",
    "print(\"\\nTranspose of x (x.T):\")\n",
    "print(x.T)\n",
    "\n",
    "# The .data attribute returns the underlying data (without gradient tracking).\n",
    "print(\"\\nUsing x.data (detached view):\")\n",
    "print(x.data)\n",
    "\n",
    "# Check if the tensor is stored on a GPU (CUDA).\n",
    "print(\"\\nIs x on CUDA? \", x.is_cuda)  # Expected to be False on most machines unless using a GPU.\n",
    "\n",
    "# MKL-DNN is a backend for CPU optimizations. Typically, default tensors are NOT MKL-DNN:\n",
    "print(\"Is x MKL-DNN? \", x.is_mkldnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46278ec9",
   "metadata": {},
   "source": [
    "### Working with Quantized and Sparse Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57151683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Tensor:\n",
      "tensor([[24.5000, 24.5000, 24.5000],\n",
      "        [19.0000, 24.5000, 24.5000],\n",
      "        [24.5000, 24.5000, 24.5000]], size=(3, 3), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n",
      "Is x_quantized quantized?  True\n",
      "\n",
      "Sparse Tensor:\n",
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([3., 4., 5.]),\n",
      "       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n",
      "Is x_sparse sparse?  True\n"
     ]
    }
   ],
   "source": [
    "# --- Quantized Tensor Example ---\n",
    "# To simulate quantization, create an integer tensor, convert to float, then quantize.\n",
    "x_int = torch.randint(low=0, high=255, size=(3, 3), dtype=torch.uint8)\n",
    "x_float = x_int.to(torch.float32)\n",
    "# Quantize the tensor: scale and zero_point are hyperparameters.\n",
    "x_quantized = torch.quantize_per_tensor(x_float, scale=0.1, zero_point=10, dtype=torch.quint8)\n",
    "\n",
    "print(\"\\nQuantized Tensor:\")\n",
    "print(x_quantized)\n",
    "print(\"Is x_quantized quantized? \", x_quantized.is_quantized)\n",
    "\n",
    "\n",
    "# --- Sparse Tensor Example ---\n",
    "# Create a sparse tensor using sparse_coo_tensor.\n",
    "indices = torch.tensor([[0, 1, 1],\n",
    "                        [2, 0, 2]])\n",
    "values = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "x_sparse = torch.sparse_coo_tensor(indices, values, size=(2, 3))\n",
    "\n",
    "print(\"\\nSparse Tensor:\")\n",
    "print(x_sparse)\n",
    "print(\"Is x_sparse sparse? \", x_sparse.is_sparse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d473ddd",
   "metadata": {},
   "source": [
    "### Demonstrate Gradient Flow on a Non-Leaf Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db3432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After backward pass:\n",
      "Gradient accumulated in x (x.grad):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "y.grad (non-leaf tensor): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_11096\\2227329156.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"y.grad (non-leaf tensor):\", y.grad)  # Remains None.\n"
     ]
    }
   ],
   "source": [
    "# Compute a simple loss by summing all elements in y.\n",
    "loss = y.sum()\n",
    "loss.backward()  # Performs backpropagation.\n",
    "\n",
    "print(\"\\nAfter backward pass:\")\n",
    "print(\"Gradient accumulated in x (x.grad):\")\n",
    "print(x.grad)      # Gradients are computed on x because of the operation that created y.\n",
    "print(\"y.grad (non-leaf tensor):\", y.grad)  # Remains None.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab284f4",
   "metadata": {},
   "source": [
    "### Additional Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a06d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Additional Methods --\n",
      "x dimensions (x.dim()): 2\n",
      "Total number of elements in x (x.numel()): 4\n",
      "x strides:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-- Additional Methods --\")\n",
    "print(\"x dimensions (x.dim()):\", x.dim())\n",
    "print(\"Total number of elements in x (x.numel()):\", x.numel())\n",
    "print(\"x strides: \", x.stride())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d5eb8a",
   "metadata": {},
   "source": [
    "# What is quantized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f38956",
   "metadata": {},
   "source": [
    "\"Quantized\" in the PyTorch context refers to a technique that converts floating-point numerical representations into lower-precision, typically integer, representations. This process is especially useful in deep learning and neural network deployment on devices with limited resources (such as mobile phones or embedded systems) because it:\n",
    "\n",
    "1.  Reduces Memory Footprint: Lower bit-width numbers (e.g., 8-bit integers instead of 32-bit floats) use significantly less memory.\n",
    "\n",
    "2.  Speeds Up Computations: Operations on lower-precision arithmetic can be faster, especially if the underlying hardware supports these operations natively.\n",
    "\n",
    "3.  Potentially Decreases Power Consumption: Lower precision arithmetic is generally more energy efficient, essential for power-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6c79d",
   "metadata": {},
   "source": [
    "### How Quantization Works\n",
    "Quantization involves mapping a wide range of floating-point values to a smaller set of integer values using two key parameters:\n",
    "\n",
    "-   Scale: A floating-point multiplier that determines how a change in the integer value corresponds to a change in the original floating-point value.\n",
    "\n",
    "-   Zero Point: An integer that ensures that zero in the floating-point range is exactly representable in the quantized space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52891fd",
   "metadata": {},
   "source": [
    "### Types of Quantization in PyTorch\n",
    "PyTorch supports several quantization techniques:\n",
    "\n",
    "-   Post-Training Quantization: Once a model has been trained, you can convert its weights (and sometimes activations) from floating-point to quantized representations. This method doesn't require retraining.\n",
    "\n",
    "-   Dynamic Quantization: Weights are quantized ahead of time, but activations are quantized on the fly during inference. This is particularly effective for models with large fully connected layers.\n",
    "\n",
    "-   Quantization-Aware Training (QAT): Here, the model is trained with simulated quantization effects so that it learns to handle the reduced numerical precision. This often leads to better accuracy in the final quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee06400",
   "metadata": {},
   "source": [
    "# We learn later more about quantized tensors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
